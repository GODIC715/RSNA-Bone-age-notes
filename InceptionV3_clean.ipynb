{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import cycle\n",
    "\n",
    "from keras import Input\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, Flatten, AveragePooling2D, concatenate\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from flow_dataframe import flow_from_dataframe\n",
    "\n",
    "# hyperparameters\n",
    "NUM_EPOCHS = 173\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE_TRAIN = 16\n",
    "BATCH_SIZE_VAL = 16\n",
    "\n",
    "\n",
    "# default size of InceptionResNetV2\n",
    "IMG_SIZE = (299, 299)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches).\n",
    "\n",
    "core_idg = ImageDataGenerator(zoom_range=0.2,\n",
    "                              fill_mode='nearest',\n",
    "                              featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                              samplewise_center=False,  # set each sample mean to 0\n",
    "                              featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                              samplewise_std_normalization=False,  # divide each input by its std\n",
    "                              zca_whitening=False,  # apply ZCA whitening\n",
    "                              rotation_range=25,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                              width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "                              height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "                              horizontal_flip=True,  # randomly flip images\n",
    "                              vertical_flip=False)\n",
    "\n",
    "val_idg = ImageDataGenerator(width_shift_range=0.25, height_shift_range=0.25, horizontal_flip=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Generators\n",
    "### Reading RSNA Boneage Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12611 images found of 12611 total\n",
      "train 10088 validation 2523\n",
      "flow_from_dataframe() -->\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Found 0 images belonging to 0 classes.\n",
      "Reinserting dataframe: 10088 images\n",
      "flow_from_dataframe() <--\n",
      "flow_from_dataframe() -->\n",
      "## Ignore next message from keras, values are replaced anyways\n",
      "Found 0 images belonging to 0 classes.\n",
      "Reinserting dataframe: 2523 images\n",
      "flow_from_dataframe() <--\n"
     ]
    }
   ],
   "source": [
    "class_str_col = 'boneage'\n",
    "gender_str_col = 'male'\n",
    "\n",
    "base_bone_dir = os.path.join('rsna-bone-age')\n",
    "boneage_df = pd.read_csv(os.path.join(base_bone_dir, 'boneage-training-dataset.csv'))\n",
    "boneage_df['path'] = boneage_df['id'].map(lambda x: os.path.join(base_bone_dir,\n",
    "                                                         'boneage-training-dataset', \n",
    "                                                         'boneage-training-dataset', \n",
    "                                                         '{}.png'.format(x)))\n",
    "\n",
    "boneage_df['exists'] = boneage_df['path'].map(os.path.exists)\n",
    "print(boneage_df['exists'].sum(), 'images found of', boneage_df.shape[0], 'total')\n",
    "\n",
    "boneage_df[gender_str_col] = boneage_df[gender_str_col].map(lambda x: np.array([1]) if x else np.array([0])) # map boolean values to 1 and 0\n",
    "\n",
    "train_df_boneage, valid_df_boneage = train_test_split(boneage_df, test_size=0.2,\n",
    "                                                      random_state=2018)  # ,stratify=boneage_df['boneage_category'])\n",
    "print('train', train_df_boneage.shape[0], 'validation', valid_df_boneage.shape[0])\n",
    "\n",
    "train_gen_boneage = flow_from_dataframe(core_idg, train_df_boneage, path_col='path', y_col=class_str_col,\n",
    "                                        target_size=IMG_SIZE,\n",
    "                                        color_mode='rgb', batch_size=BATCH_SIZE_TRAIN)\n",
    "\n",
    "# used a fixed dataset for evaluating the algorithm\n",
    "valid_gen_boneage = flow_from_dataframe(core_idg, valid_df_boneage, path_col='path', y_col=class_str_col,\n",
    "                                        target_size=IMG_SIZE,\n",
    "                                        color_mode='rgb',\n",
    "                                        batch_size=BATCH_SIZE_VAL)  # we can use much larger batches for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = Input(shape=(299, 299, 3), name='input_img')\n",
    "i2 = Input(shape=(1), name='input_gender')\n",
    "base = InceptionV3(input_tensor=i1, input_shape=(299, 299, 3), include_top=False, weights=None)\n",
    "\n",
    "feature_img = base.get_layer(name='mixed10').output\n",
    "feature_img = AveragePooling2D((2, 2))(feature_img)\n",
    "feature_img = Flatten()(feature_img)\n",
    "feature_gender = Dense(32, activation='relu')(i2)\n",
    "feature = concatenate([feature_img, feature_gender], axis=1)\n",
    "\n",
    "o = Dense(1000, activation='relu')(feature)\n",
    "o = Dense(1000, activation='relu')(o)\n",
    "o = Dense(1)(o)\n",
    "model = Model(inputs=[i1, i2], outputs=o)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model on Boneage Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = \"{}_weights.best.hdf5\".format('bone_age')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1,\n",
    "                            save_best_only=True, mode='min', save_weights_only=True)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
    "                      patience=10)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=15, verbose=1,\n",
    "                                   save_best_only=True, mode='auto', min_delta=0.0001, cooldown=5)\n",
    "\n",
    "\n",
    "def combined_generators(image_generator, gender_data, batch_size):\n",
    "    gender_generator = cycle(batch(gender_data, batch_size))\n",
    "    while True:\n",
    "        nextImage = next(image_generator)\n",
    "        nextGender = next(gender_generator)\n",
    "        nextGender=np.asarray(nextGender).astype(int)\n",
    "        assert len(nextImage[0]) == len(nextGender)\n",
    "        yield [nextImage[0], nextGender], nextImage[1]\n",
    "\n",
    "\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "train_gen_wrapper = combined_generators(train_gen_boneage, train_df_boneage[gender_str_col], BATCH_SIZE_TRAIN)\n",
    "val_gen_wrapper = combined_generators(valid_gen_boneage, valid_df_boneage[gender_str_col], BATCH_SIZE_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 36.0266 - mae: 36.0266\n",
      "Epoch 00001: val_loss improved from inf to 39.36032, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 906s 1s/step - loss: 36.0266 - mae: 36.0266 - val_loss: 39.3603 - val_mae: 39.3603 - lr: 0.0010\n",
      "Epoch 2/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 34.0375 - mae: 34.0375\n",
      "Epoch 00002: val_loss improved from 39.36032 to 36.11832, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 844s 1s/step - loss: 34.0375 - mae: 34.0375 - val_loss: 36.1183 - val_mae: 36.1183 - lr: 0.0010\n",
      "Epoch 3/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 33.5279 - mae: 33.5279\n",
      "Epoch 00003: val_loss did not improve from 36.11832\n",
      "631/631 [==============================] - 771s 1s/step - loss: 33.5279 - mae: 33.5279 - val_loss: 45.0889 - val_mae: 45.0889 - lr: 0.0010\n",
      "Epoch 4/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 32.7572 - mae: 32.7572\n",
      "Epoch 00004: val_loss improved from 36.11832 to 33.89449, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 718s 1s/step - loss: 32.7572 - mae: 32.7572 - val_loss: 33.8945 - val_mae: 33.8945 - lr: 0.0010\n",
      "Epoch 5/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 32.1663 - mae: 32.1663\n",
      "Epoch 00005: val_loss improved from 33.89449 to 32.12285, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 706s 1s/step - loss: 32.1663 - mae: 32.1663 - val_loss: 32.1228 - val_mae: 32.1228 - lr: 0.0010\n",
      "Epoch 6/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 31.4606 - mae: 31.4606\n",
      "Epoch 00006: val_loss improved from 32.12285 to 31.19760, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 780s 1s/step - loss: 31.4606 - mae: 31.4606 - val_loss: 31.1976 - val_mae: 31.1976 - lr: 0.0010\n",
      "Epoch 7/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 31.2237 - mae: 31.2237\n",
      "Epoch 00007: val_loss improved from 31.19760 to 30.90422, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 711s 1s/step - loss: 31.2237 - mae: 31.2237 - val_loss: 30.9042 - val_mae: 30.9042 - lr: 0.0010\n",
      "Epoch 8/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 29.6962 - mae: 29.6962\n",
      "Epoch 00008: val_loss did not improve from 30.90422\n",
      "631/631 [==============================] - 706s 1s/step - loss: 29.6962 - mae: 29.6962 - val_loss: 33.8470 - val_mae: 33.8470 - lr: 0.0010\n",
      "Epoch 9/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 26.8097 - mae: 26.8097\n",
      "Epoch 00009: val_loss did not improve from 30.90422\n",
      "631/631 [==============================] - 654s 1s/step - loss: 26.8097 - mae: 26.8097 - val_loss: 46.7356 - val_mae: 46.7356 - lr: 0.0010\n",
      "Epoch 10/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 23.7545 - mae: 23.7545\n",
      "Epoch 00010: val_loss improved from 30.90422 to 28.51280, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 673s 1s/step - loss: 23.7545 - mae: 23.7545 - val_loss: 28.5128 - val_mae: 28.5128 - lr: 0.0010\n",
      "Epoch 11/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 22.2620 - mae: 22.2620\n",
      "Epoch 00011: val_loss improved from 28.51280 to 22.69400, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 722s 1s/step - loss: 22.2620 - mae: 22.2620 - val_loss: 22.6940 - val_mae: 22.6940 - lr: 0.0010\n",
      "Epoch 12/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 20.9633 - mae: 20.9633\n",
      "Epoch 00012: val_loss did not improve from 22.69400\n",
      "631/631 [==============================] - 710s 1s/step - loss: 20.9633 - mae: 20.9633 - val_loss: 24.9490 - val_mae: 24.9490 - lr: 0.0010\n",
      "Epoch 13/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 18.7990 - mae: 18.7990\n",
      "Epoch 00013: val_loss improved from 22.69400 to 20.25049, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 654s 1s/step - loss: 18.7990 - mae: 18.7990 - val_loss: 20.2505 - val_mae: 20.2505 - lr: 0.0010\n",
      "Epoch 14/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 18.6810 - mae: 18.6810\n",
      "Epoch 00014: val_loss did not improve from 20.25049\n",
      "631/631 [==============================] - 662s 1s/step - loss: 18.6810 - mae: 18.6810 - val_loss: 21.0652 - val_mae: 21.0652 - lr: 0.0010\n",
      "Epoch 15/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 17.6819 - mae: 17.6819\n",
      "Epoch 00015: val_loss improved from 20.25049 to 16.80867, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 651s 1s/step - loss: 17.6819 - mae: 17.6819 - val_loss: 16.8087 - val_mae: 16.8087 - lr: 0.0010\n",
      "Epoch 16/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 17.6223 - mae: 17.6223\n",
      "Epoch 00016: val_loss did not improve from 16.80867\n",
      "631/631 [==============================] - 667s 1s/step - loss: 17.6223 - mae: 17.6223 - val_loss: 21.1384 - val_mae: 21.1384 - lr: 0.0010\n",
      "Epoch 17/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 16.9533 - mae: 16.9533\n",
      "Epoch 00017: val_loss improved from 16.80867 to 16.02236, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 743s 1s/step - loss: 16.9533 - mae: 16.9533 - val_loss: 16.0224 - val_mae: 16.0224 - lr: 0.0010\n",
      "Epoch 18/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 16.3577 - mae: 16.3577\n",
      "Epoch 00018: val_loss did not improve from 16.02236\n",
      "631/631 [==============================] - 668s 1s/step - loss: 16.3577 - mae: 16.3577 - val_loss: 17.5419 - val_mae: 17.5419 - lr: 0.0010\n",
      "Epoch 19/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 16.2198 - mae: 16.2198\n",
      "Epoch 00019: val_loss did not improve from 16.02236\n",
      "631/631 [==============================] - 648s 1s/step - loss: 16.2198 - mae: 16.2198 - val_loss: 21.8789 - val_mae: 21.8789 - lr: 0.0010\n",
      "Epoch 20/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 15.7591 - mae: 15.7591\n",
      "Epoch 00020: val_loss did not improve from 16.02236\n",
      "631/631 [==============================] - 647s 1s/step - loss: 15.7591 - mae: 15.7591 - val_loss: 19.5769 - val_mae: 19.5769 - lr: 0.0010\n",
      "Epoch 21/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 15.6620 - mae: 15.6620\n",
      "Epoch 00021: val_loss did not improve from 16.02236\n",
      "631/631 [==============================] - 652s 1s/step - loss: 15.6620 - mae: 15.6620 - val_loss: 21.5324 - val_mae: 21.5324 - lr: 0.0010\n",
      "Epoch 22/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 15.4700 - mae: 15.4700\n",
      "Epoch 00022: val_loss did not improve from 16.02236\n",
      "631/631 [==============================] - 748s 1s/step - loss: 15.4700 - mae: 15.4700 - val_loss: 17.7684 - val_mae: 17.7684 - lr: 0.0010\n",
      "Epoch 23/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 14.9264 - mae: 14.9264\n",
      "Epoch 00023: val_loss improved from 16.02236 to 15.88217, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 653s 1s/step - loss: 14.9264 - mae: 14.9264 - val_loss: 15.8822 - val_mae: 15.8822 - lr: 0.0010\n",
      "Epoch 24/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 14.9588 - mae: 14.9588\n",
      "Epoch 00024: val_loss did not improve from 15.88217\n",
      "631/631 [==============================] - 666s 1s/step - loss: 14.9588 - mae: 14.9588 - val_loss: 17.1755 - val_mae: 17.1755 - lr: 0.0010\n",
      "Epoch 25/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 14.7083 - mae: 14.7083\n",
      "Epoch 00025: val_loss improved from 15.88217 to 14.55395, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 660s 1s/step - loss: 14.7083 - mae: 14.7083 - val_loss: 14.5539 - val_mae: 14.5539 - lr: 0.0010\n",
      "Epoch 26/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 14.3430 - mae: 14.3430\n",
      "Epoch 00026: val_loss improved from 14.55395 to 14.05094, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 674s 1s/step - loss: 14.3430 - mae: 14.3430 - val_loss: 14.0509 - val_mae: 14.0509 - lr: 0.0010\n",
      "Epoch 27/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 13.8942 - mae: 13.8942\n",
      "Epoch 00027: val_loss did not improve from 14.05094\n",
      "631/631 [==============================] - 723s 1s/step - loss: 13.8942 - mae: 13.8942 - val_loss: 18.1936 - val_mae: 18.1936 - lr: 0.0010\n",
      "Epoch 28/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 14.0314 - mae: 14.0314\n",
      "Epoch 00028: val_loss did not improve from 14.05094\n",
      "631/631 [==============================] - 692s 1s/step - loss: 14.0314 - mae: 14.0314 - val_loss: 15.9123 - val_mae: 15.9123 - lr: 0.0010\n",
      "Epoch 29/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 13.9583 - mae: 13.9583\n",
      "Epoch 00029: val_loss did not improve from 14.05094\n",
      "631/631 [==============================] - 659s 1s/step - loss: 13.9583 - mae: 13.9583 - val_loss: 21.5393 - val_mae: 21.5393 - lr: 0.0010\n",
      "Epoch 30/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 14.0129 - mae: 14.0129\n",
      "Epoch 00030: val_loss did not improve from 14.05094\n",
      "631/631 [==============================] - 661s 1s/step - loss: 14.0129 - mae: 14.0129 - val_loss: 17.9810 - val_mae: 17.9810 - lr: 0.0010\n",
      "Epoch 31/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 13.6777 - mae: 13.6777\n",
      "Epoch 00031: val_loss did not improve from 14.05094\n",
      "631/631 [==============================] - 657s 1s/step - loss: 13.6777 - mae: 13.6777 - val_loss: 16.5135 - val_mae: 16.5135 - lr: 0.0010\n",
      "Epoch 32/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 13.4787 - mae: 13.4787\n",
      "Epoch 00032: val_loss did not improve from 14.05094\n",
      "631/631 [==============================] - 659s 1s/step - loss: 13.4787 - mae: 13.4787 - val_loss: 26.6828 - val_mae: 26.6828 - lr: 0.0010\n",
      "Epoch 33/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 13.6353 - mae: 13.6353\n",
      "Epoch 00033: val_loss did not improve from 14.05094\n",
      "631/631 [==============================] - 736s 1s/step - loss: 13.6353 - mae: 13.6353 - val_loss: 14.3018 - val_mae: 14.3018 - lr: 0.0010\n",
      "Epoch 34/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 13.1847 - mae: 13.1847\n",
      "Epoch 00034: val_loss did not improve from 14.05094\n",
      "631/631 [==============================] - 657s 1s/step - loss: 13.1847 - mae: 13.1847 - val_loss: 14.7814 - val_mae: 14.7814 - lr: 0.0010\n",
      "Epoch 35/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 13.2881 - mae: 13.2881\n",
      "Epoch 00035: val_loss improved from 14.05094 to 13.33056, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 660s 1s/step - loss: 13.2881 - mae: 13.2881 - val_loss: 13.3306 - val_mae: 13.3306 - lr: 0.0010\n",
      "Epoch 36/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.9721 - mae: 12.9721\n",
      "Epoch 00036: val_loss did not improve from 13.33056\n",
      "631/631 [==============================] - 671s 1s/step - loss: 12.9721 - mae: 12.9721 - val_loss: 14.9031 - val_mae: 14.9031 - lr: 0.0010\n",
      "Epoch 37/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.9220 - mae: 12.9220\n",
      "Epoch 00037: val_loss did not improve from 13.33056\n",
      "631/631 [==============================] - 661s 1s/step - loss: 12.9220 - mae: 12.9220 - val_loss: 14.5101 - val_mae: 14.5101 - lr: 0.0010\n",
      "Epoch 38/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.8535 - mae: 12.8535\n",
      "Epoch 00038: val_loss did not improve from 13.33056\n",
      "631/631 [==============================] - 751s 1s/step - loss: 12.8535 - mae: 12.8535 - val_loss: 13.3397 - val_mae: 13.3397 - lr: 0.0010\n",
      "Epoch 39/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.7752 - mae: 12.7752\n",
      "Epoch 00039: val_loss did not improve from 13.33056\n",
      "631/631 [==============================] - 653s 1s/step - loss: 12.7752 - mae: 12.7752 - val_loss: 13.3943 - val_mae: 13.3943 - lr: 0.0010\n",
      "Epoch 40/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.7652 - mae: 12.7652\n",
      "Epoch 00040: val_loss did not improve from 13.33056\n",
      "631/631 [==============================] - 648s 1s/step - loss: 12.7652 - mae: 12.7652 - val_loss: 23.7457 - val_mae: 23.7457 - lr: 0.0010\n",
      "Epoch 41/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.7595 - mae: 12.7595\n",
      "Epoch 00041: val_loss did not improve from 13.33056\n",
      "631/631 [==============================] - 652s 1s/step - loss: 12.7595 - mae: 12.7595 - val_loss: 18.3258 - val_mae: 18.3258 - lr: 0.0010\n",
      "Epoch 42/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.5493 - mae: 12.5493\n",
      "Epoch 00042: val_loss did not improve from 13.33056\n",
      "631/631 [==============================] - 653s 1s/step - loss: 12.5493 - mae: 12.5493 - val_loss: 16.2656 - val_mae: 16.2656 - lr: 0.0010\n",
      "Epoch 43/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.3697 - mae: 12.3697\n",
      "Epoch 00043: val_loss improved from 13.33056 to 13.15841, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 707s 1s/step - loss: 12.3697 - mae: 12.3697 - val_loss: 13.1584 - val_mae: 13.1584 - lr: 0.0010\n",
      "Epoch 44/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.3440 - mae: 12.3440\n",
      "Epoch 00044: val_loss improved from 13.15841 to 12.85427, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 702s 1s/step - loss: 12.3440 - mae: 12.3440 - val_loss: 12.8543 - val_mae: 12.8543 - lr: 0.0010\n",
      "Epoch 45/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.1514 - mae: 12.1514\n",
      "Epoch 00045: val_loss did not improve from 12.85427\n",
      "631/631 [==============================] - 663s 1s/step - loss: 12.1514 - mae: 12.1514 - val_loss: 13.5561 - val_mae: 13.5561 - lr: 0.0010\n",
      "Epoch 46/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.1542 - mae: 12.1542\n",
      "Epoch 00046: val_loss improved from 12.85427 to 12.54917, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 657s 1s/step - loss: 12.1542 - mae: 12.1542 - val_loss: 12.5492 - val_mae: 12.5492 - lr: 0.0010\n",
      "Epoch 47/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.2816 - mae: 12.2816\n",
      "Epoch 00047: val_loss did not improve from 12.54917\n",
      "631/631 [==============================] - 671s 1s/step - loss: 12.2816 - mae: 12.2816 - val_loss: 13.1348 - val_mae: 13.1348 - lr: 0.0010\n",
      "Epoch 48/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.0193 - mae: 12.0193\n",
      "Epoch 00048: val_loss did not improve from 12.54917\n",
      "631/631 [==============================] - 642s 1s/step - loss: 12.0193 - mae: 12.0193 - val_loss: 13.4837 - val_mae: 13.4837 - lr: 0.0010\n",
      "Epoch 49/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 12.0402 - mae: 12.0402\n",
      "Epoch 00049: val_loss did not improve from 12.54917\n",
      "631/631 [==============================] - 703s 1s/step - loss: 12.0402 - mae: 12.0402 - val_loss: 12.9455 - val_mae: 12.9455 - lr: 0.0010\n",
      "Epoch 50/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.8508 - mae: 11.8508\n",
      "Epoch 00050: val_loss did not improve from 12.54917\n",
      "631/631 [==============================] - 591s 937ms/step - loss: 11.8508 - mae: 11.8508 - val_loss: 12.8944 - val_mae: 12.8944 - lr: 0.0010\n",
      "Epoch 51/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.8749 - mae: 11.8749\n",
      "Epoch 00051: val_loss did not improve from 12.54917\n",
      "631/631 [==============================] - 594s 942ms/step - loss: 11.8749 - mae: 11.8749 - val_loss: 26.5885 - val_mae: 26.5885 - lr: 0.0010\n",
      "Epoch 52/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.9191 - mae: 11.9191\n",
      "Epoch 00052: val_loss did not improve from 12.54917\n",
      "631/631 [==============================] - 590s 936ms/step - loss: 11.9191 - mae: 11.9191 - val_loss: 13.0184 - val_mae: 13.0184 - lr: 0.0010\n",
      "Epoch 53/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.6357 - mae: 11.6357\n",
      "Epoch 00053: val_loss improved from 12.54917 to 12.46458, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 598s 948ms/step - loss: 11.6357 - mae: 11.6357 - val_loss: 12.4646 - val_mae: 12.4646 - lr: 0.0010\n",
      "Epoch 54/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.6744 - mae: 11.6744\n",
      "Epoch 00054: val_loss did not improve from 12.46458\n",
      "631/631 [==============================] - 666s 1s/step - loss: 11.6744 - mae: 11.6744 - val_loss: 13.2132 - val_mae: 13.2132 - lr: 0.0010\n",
      "Epoch 55/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.5903 - mae: 11.5903\n",
      "Epoch 00055: val_loss did not improve from 12.46458\n",
      "631/631 [==============================] - 694s 1s/step - loss: 11.5903 - mae: 11.5903 - val_loss: 14.1249 - val_mae: 14.1249 - lr: 0.0010\n",
      "Epoch 56/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.7544 - mae: 11.7544\n",
      "Epoch 00056: val_loss did not improve from 12.46458\n",
      "631/631 [==============================] - 606s 961ms/step - loss: 11.7544 - mae: 11.7544 - val_loss: 13.1227 - val_mae: 13.1227 - lr: 0.0010\n",
      "Epoch 57/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.5218 - mae: 11.5218\n",
      "Epoch 00057: val_loss improved from 12.46458 to 11.68500, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 607s 963ms/step - loss: 11.5218 - mae: 11.5218 - val_loss: 11.6850 - val_mae: 11.6850 - lr: 0.0010\n",
      "Epoch 58/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.5506 - mae: 11.5506\n",
      "Epoch 00058: val_loss did not improve from 11.68500\n",
      "631/631 [==============================] - 624s 990ms/step - loss: 11.5506 - mae: 11.5506 - val_loss: 12.7543 - val_mae: 12.7543 - lr: 0.0010\n",
      "Epoch 59/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.3641 - mae: 11.3641\n",
      "Epoch 00059: val_loss did not improve from 11.68500\n",
      "631/631 [==============================] - 602s 954ms/step - loss: 11.3641 - mae: 11.3641 - val_loss: 11.9636 - val_mae: 11.9636 - lr: 0.0010\n",
      "Epoch 60/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.3802 - mae: 11.3802\n",
      "Epoch 00060: val_loss did not improve from 11.68500\n",
      "631/631 [==============================] - 701s 1s/step - loss: 11.3802 - mae: 11.3802 - val_loss: 12.9732 - val_mae: 12.9732 - lr: 0.0010\n",
      "Epoch 61/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.3176 - mae: 11.3176\n",
      "Epoch 00061: val_loss did not improve from 11.68500\n",
      "631/631 [==============================] - 622s 987ms/step - loss: 11.3176 - mae: 11.3176 - val_loss: 13.2171 - val_mae: 13.2171 - lr: 0.0010\n",
      "Epoch 62/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.3829 - mae: 11.3829\n",
      "Epoch 00062: val_loss did not improve from 11.68500\n",
      "631/631 [==============================] - 603s 957ms/step - loss: 11.3829 - mae: 11.3829 - val_loss: 12.2905 - val_mae: 12.2905 - lr: 0.0010\n",
      "Epoch 63/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.2480 - mae: 11.2480\n",
      "Epoch 00063: val_loss did not improve from 11.68500\n",
      "631/631 [==============================] - 608s 965ms/step - loss: 11.2480 - mae: 11.2480 - val_loss: 12.3708 - val_mae: 12.3708 - lr: 0.0010\n",
      "Epoch 64/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.2165 - mae: 11.2165\n",
      "Epoch 00064: val_loss did not improve from 11.68500\n",
      "631/631 [==============================] - 595s 944ms/step - loss: 11.2165 - mae: 11.2165 - val_loss: 14.0114 - val_mae: 14.0114 - lr: 0.0010\n",
      "Epoch 65/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.2333 - mae: 11.2333\n",
      "Epoch 00065: val_loss did not improve from 11.68500\n",
      "631/631 [==============================] - 599s 951ms/step - loss: 11.2333 - mae: 11.2333 - val_loss: 12.7255 - val_mae: 12.7255 - lr: 0.0010\n",
      "Epoch 66/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.1558 - mae: 11.1558\n",
      "Epoch 00066: val_loss improved from 11.68500 to 11.58469, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 724s 1s/step - loss: 11.1558 - mae: 11.1558 - val_loss: 11.5847 - val_mae: 11.5847 - lr: 0.0010\n",
      "Epoch 67/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.1856 - mae: 11.1856\n",
      "Epoch 00067: val_loss did not improve from 11.58469\n",
      "631/631 [==============================] - 622s 987ms/step - loss: 11.1856 - mae: 11.1856 - val_loss: 14.4338 - val_mae: 14.4338 - lr: 0.0010\n",
      "Epoch 68/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.1228 - mae: 11.1228\n",
      "Epoch 00068: val_loss did not improve from 11.58469\n",
      "631/631 [==============================] - 599s 951ms/step - loss: 11.1228 - mae: 11.1228 - val_loss: 13.0316 - val_mae: 13.0316 - lr: 0.0010\n",
      "Epoch 69/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.0453 - mae: 11.0453\n",
      "Epoch 00069: val_loss did not improve from 11.58469\n",
      "631/631 [==============================] - 600s 952ms/step - loss: 11.0453 - mae: 11.0453 - val_loss: 11.8651 - val_mae: 11.8651 - lr: 0.0010\n",
      "Epoch 70/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 11.0240 - mae: 11.0240\n",
      "Epoch 00070: val_loss did not improve from 11.58469\n",
      "631/631 [==============================] - 598s 949ms/step - loss: 11.0240 - mae: 11.0240 - val_loss: 11.7555 - val_mae: 11.7555 - lr: 0.0010\n",
      "Epoch 71/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 10.9471 - mae: 10.9471\n",
      "Epoch 00071: val_loss improved from 11.58469 to 11.41437, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 612s 971ms/step - loss: 10.9471 - mae: 10.9471 - val_loss: 11.4144 - val_mae: 11.4144 - lr: 0.0010\n",
      "Epoch 72/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 10.9616 - mae: 10.9616\n",
      "Epoch 00072: val_loss did not improve from 11.41437\n",
      "631/631 [==============================] - 724s 1s/step - loss: 10.9616 - mae: 10.9616 - val_loss: 12.8262 - val_mae: 12.8262 - lr: 0.0010\n",
      "Epoch 73/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 10.9259 - mae: 10.9259\n",
      "Epoch 00073: val_loss improved from 11.41437 to 11.34369, saving model to bone_age_weights.best.hdf5\n",
      "631/631 [==============================] - 608s 964ms/step - loss: 10.9259 - mae: 10.9259 - val_loss: 11.3437 - val_mae: 11.3437 - lr: 0.0010\n",
      "Epoch 74/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 10.8437 - mae: 10.8437\n",
      "Epoch 00074: val_loss did not improve from 11.34369\n",
      "631/631 [==============================] - 640s 1s/step - loss: 10.8437 - mae: 10.8437 - val_loss: 13.7634 - val_mae: 13.7634 - lr: 0.0010\n",
      "Epoch 75/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 10.7782 - mae: 10.7782\n",
      "Epoch 00075: val_loss did not improve from 11.34369\n",
      "631/631 [==============================] - 619s 983ms/step - loss: 10.7782 - mae: 10.7782 - val_loss: 13.9634 - val_mae: 13.9634 - lr: 0.0010\n",
      "Epoch 76/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 10.7675 - mae: 10.7675\n",
      "Epoch 00076: val_loss did not improve from 11.34369\n",
      "631/631 [==============================] - 617s 978ms/step - loss: 10.7675 - mae: 10.7675 - val_loss: 11.4021 - val_mae: 11.4021 - lr: 0.0010\n",
      "Epoch 77/250\n",
      "631/631 [==============================] - ETA: 0s - loss: 10.7619 - mae: 10.7619\n",
      "Epoch 00077: val_loss did not improve from 11.34369\n",
      "631/631 [==============================] - 690s 1s/step - loss: 10.7619 - mae: 10.7619 - val_loss: 13.0181 - val_mae: 13.0181 - lr: 0.0010\n",
      "Epoch 78/250\n",
      "249/631 [==========>...................] - ETA: 5:18 - loss: 10.6999 - mae: 10.6999"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17252/882777456.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = model.fit(train_gen_wrapper, validation_data=val_gen_wrapper,\n\u001b[0m\u001b[0;32m      2\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen_boneage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_gen_boneage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                               callbacks=[early, reduceLROnPlat, checkpoint])\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\College and Courses\\College\\GRAD PROJECT\\newEnv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\College and Courses\\College\\GRAD PROJECT\\newEnv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\College and Courses\\College\\GRAD PROJECT\\newEnv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\College and Courses\\College\\GRAD PROJECT\\newEnv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\College and Courses\\College\\GRAD PROJECT\\newEnv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\College and Courses\\College\\GRAD PROJECT\\newEnv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\College and Courses\\College\\GRAD PROJECT\\newEnv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32me:\\College and Courses\\College\\GRAD PROJECT\\newEnv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\College and Courses\\College\\GRAD PROJECT\\newEnv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_gen_wrapper, validation_data=val_gen_wrapper,\n",
    "                              epochs=NUM_EPOCHS, steps_per_epoch=len(train_gen_boneage),\n",
    "                              validation_steps=len(valid_gen_boneage),\n",
    "                              callbacks=[early, reduceLROnPlat, checkpoint])\n",
    "\n",
    "model.save('saved_model.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5898fd49d3a85f7f6a1728e6c8cd77fe1356b68e4a2df5dc09cb8f4bba0dc5f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
